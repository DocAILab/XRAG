[api_keys]
api_key = "sk-xxxx"
api_base = "https://api.openai.com/v1"
api_name = "gpt-4o"
auth_token = "hf_xxx"


[logging]
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level = "INFO"
# Path to log file - comment or set to empty string to disable file logging
log_file = "logs/xrag.log"
# Log format
log_format = "[%(asctime)s,%(msecs)03d] %(levelname)-8s - (%(name)s) %(filename)s, ln %(lineno)d: %(message)s"


[settings]
llm = "openai" # openai, huggingface, ollama
ollama_model = "llama2:7b" # ollama model name
huggingface_model = "llama" # huggingface model name
embeddings = "BAAI/bge-large-en-v1.5"
split_type = "sentence"
chunk_size = 128
dataset = "hotpot_qa"
# dataset_path = "data/xxx.json" # If dataset is custom, set the path to the custom dataset
persist_dir = "storage"
llamaIndexEvaluateModel = "Qwen/Qwen1.5-7B-Chat-GPTQ-Int8"
deepEvalEvaluateModel = "Qwen/Qwen1.5-7B-Chat-GPTQ-Int8"
upTrainEvaluateModel = "qwen:7b-chat-v1.5-q8_0"
evaluateApiName = ""
evaluateApiKey = ""
evaluateApiBase = ""
output = ""
n = 100
test_init_total_number_documents = 20
extra_number_documents = 20
extra_rate_documents = 0.1
test_all_number_documents = 40
experiment_1 = false
retriever = "BM25"
retriever_mode = 0
postprocess_rerank = "long_context_reorder"
query_transform = "none"
metrics = ["NLG_chrf", "NLG_meteor", "NLG_wer", "NLG_cer", "NLG_chrf_pp","NLG_perplexity", "NLG_rouge_rouge1", "NLG_rouge_rouge2", "NLG_rouge_rougeL", "NLG_rouge_rougeLsum"]

[prompt]
text_qa_template_path = "src/xrag/prompts/text_qa_template.txt"
refine_template_path = "src/xrag/prompts/refine_template.txt"
