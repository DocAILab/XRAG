[llm_settings]
llm = "ollama"
# openai setting(when llm is openai)
api_key = "sk-*****"
api_base = "https://api.deepseek.com/v1"  # https://api.openai.com/v1
api_name = "deepseek-chat" # gpt-4o deepseek-chat
auth_token = "hf_xxx"
temperature = 0
# huggingface setting(when llm is huggingface)
huggingface_model = "llama"
# ollama setting(when llm is ollama)
ollama_model = "deepseek-r1:1.5b"
ollama_request_timeout = 60



[embedding_settings]
embedding_type = "local"  #or huggingface
embeddings = "BAAI/llm-embedder" # BAAI/llm-embedder BAAI/bge-large-en-v1.5
embed_batch_size = 16




[chunk_settings]
split_type = "sentence" #sentence,sentence_window,character,hierarchical
# when split_type is sentence
chunk_size = 128
chunk_overlap = 20
# when split_type is sentence_window
window_size = 3
# when split_type is hierarchical
chunk_sizes=[2048, 512, 128]
# when split_type is semantic
buffer_size = 1
include_metadata = true
include_prev_next_rel = true 
breakpoint_percentile_threshold = 95



[dataset_settings]
dataset_type = "local"
persist_dir = "storage"
# if dataset_type is huggingface
dataset = "hotpot_qa"
dataset_download_timeout = 300 # seconds
# if dataset_type is local
dataset_path = "examples/generated_qa.json"



[Evaluate_settings]
# 这一块都是实验部分用到的参数
llamaIndexEvaluateModel = "Qwen/Qwen1.5-7B-Chat-GPTQ-Int8"
deepEvalEvaluateModel = "Qwen/Qwen1.5-7B-Chat-GPTQ-Int8"
upTrainEvaluateModel = "qwen:7b-chat-v1.5-q8_0"
evaluateApiName = ""
evaluateApiKey = ""
evaluateApiBase = ""
output = ""
n = 100
test_init_total_number_documents = 20
extra_number_documents = 20
extra_rate_documents = 0.1
test_all_number_documents = 40
experiment_1 = false

[responce_synthsizer]
# 回答合成器设
# 这里原来是一个数字的，我把他优化到一个字符串，并且支持大小写不敏感,如果输入的字符串不支持，则采用默认的refine
# refine,compact,compact_accumulate,accumulate,tree_summarize,simple_summarize,no_text,generation
responce_synthsizer = 'refine'

[retrieval_settings]
# 这里非常复杂，有10种retriver
retriever = "BM25"
# 1.BM25
similarity_top_k_BM25=3

# 2.Vector
similarity_top_k_VECTOR=3
show_progress_VECTOR=true
shore_nodes_override_VECTOR=true

# 3.Summary
retriver_type_SUMMARY='normal' # normal,embed,llm
similarity_top_k_SUMMARY=3

# 4.Tree
retriver_type_TREE='root' # root allleaf selectleaf selectleafembedding

# 5.Keyword
# 不需要设置

# 6.Custom 
retriver_type_CUSTOM='bm25_and' # bm25_and,keyword_or

# 7.QueryFusion
retriver_type_QUERYFUSION='normal' # normal,reciprocal_rank
num_quries_QUERYFUSION=4
similarity_top_k_QUERYFUSION=2
retriever_weight_QUERYFUSION=[]  # default: [1 / len(index)] * len(index)

# 8.AutoMerging
similarity_top_k_AUTOMERGING=6

# 9.Recursive
sub_chunk_sizes_RECURSIVE=[128,256,512]
chunk_overlap_RECURSIVE=20
similarity_top_k_RECURSIVE=3

# 10.SentenceWindow
# 无需设置

[postprocessor_setting]
# 会根据这个选项构造合适的后处理器
postprocess_rerank = "long_context_reorder" 



[query_settings]
query_transform = "none"
metrics = ["NLG_chrf", "NLG_meteor", "NLG_wer", "NLG_cer", "NLG_chrf_pp","NLG_perplexity", "NLG_rouge_rouge1", "NLG_rouge_rouge2", "NLG_rouge_rougeL", "NLG_rouge_rougeLsum", "SePer_with_context", "SePer_without_context", "SePer_delta"]


[seper]
# SePer (Semantic Perplexity) metric configuration
enabled = false
generation_model = "meta-llama/Llama-2-7b-chat-hf"
entailment_model = "microsoft/deberta-v2-xlarge-mnli"
device = "cuda"
num_generations = 10
temperature = 1.0
max_new_tokens = 128
computation_chunk_size = 8
prompt_type = "default"
max_context_words = 512
use_soft_clustering = true


[prompt]
text_qa_template_path = "src/xrag/prompts/text_qa_template.txt"
refine_template_path = "src/xrag/prompts/refine_template.txt"


[self_rag]
enabled = false # when enabled, ignore other settings and use self-rag
model_name = "selfrag/selfrag_llama2_7b"
